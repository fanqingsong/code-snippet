{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "    categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(twenty_train.data))\n",
    "\n",
    "len(twenty_train.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "--------\n",
      "comp.graphics\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\n",
    "\n",
    "print(\"--------\")\n",
    "\n",
    "\n",
    "print(twenty_train.target_names[twenty_train.target[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n",
      "comp.graphics\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "sci.med\n",
      "sci.med\n",
      "sci.med\n"
     ]
    }
   ],
   "source": [
    "for t in twenty_train.target[:10]:\n",
    "    print(twenty_train.target_names[t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2257, 35788)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "from\n",
      "  (0, 14887)\t1\n",
      "  (0, 29022)\t1\n",
      "  (0, 8696)\t4\n",
      "  (0, 4017)\t2\n",
      "  (0, 33256)\t2\n",
      "  (0, 21661)\t3\n",
      "  (0, 9031)\t3\n",
      "  (0, 31077)\t1\n",
      "  (0, 9805)\t2\n",
      "  (0, 17366)\t1\n",
      "  (0, 32493)\t4\n",
      "  (0, 16916)\t2\n",
      "  (0, 19780)\t2\n",
      "  (0, 17302)\t2\n",
      "  (0, 23122)\t1\n",
      "  (0, 25663)\t1\n",
      "  (0, 16881)\t1\n",
      "  (0, 16082)\t1\n",
      "  (0, 23915)\t1\n",
      "  (0, 32142)\t5\n",
      "  (0, 33597)\t2\n",
      "  (0, 20253)\t1\n",
      "  (0, 587)\t1\n",
      "  (0, 12051)\t1\n",
      "  (0, 5201)\t1\n",
      "  :\t:\n",
      "  (2256, 13740)\t1\n",
      "  (2256, 14662)\t1\n",
      "  (2256, 20201)\t1\n",
      "  (2256, 12443)\t6\n",
      "  (2256, 30325)\t3\n",
      "  (2256, 4610)\t1\n",
      "  (2256, 33844)\t1\n",
      "  (2256, 17354)\t1\n",
      "  (2256, 26998)\t1\n",
      "  (2256, 20277)\t1\n",
      "  (2256, 20695)\t1\n",
      "  (2256, 20702)\t1\n",
      "  (2256, 9649)\t1\n",
      "  (2256, 9086)\t1\n",
      "  (2256, 26254)\t1\n",
      "  (2256, 17133)\t2\n",
      "  (2256, 4490)\t1\n",
      "  (2256, 13720)\t1\n",
      "  (2256, 5016)\t1\n",
      "  (2256, 9632)\t1\n",
      "  (2256, 11824)\t1\n",
      "  (2256, 29993)\t1\n",
      "  (2256, 1298)\t1\n",
      "  (2256, 2375)\t1\n",
      "  (2256, 3921)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "\n",
    "# Printing the identified Unique words along with their indices \n",
    "#print(\"Vocabulary: \", count_vect.vocabulary_) \n",
    "\n",
    "print(X_train_counts.shape)\n",
    "print(type(X_train_counts))\n",
    "print(count_vect.get_feature_names()[14887])\n",
    "\n",
    "print(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "  (0, 8)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t2\n",
      "  (1, 5)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "['and this', 'document is', 'first document', 'is the', 'is this', 'second document', 'the first', 'the second', 'the third', 'third one', 'this document', 'this is', 'this the']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X)\n",
    "print(X.toarray())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2257, 35788)\n",
      "  (0, 177)\t0.15075567228888181\n",
      "  (0, 230)\t0.07537783614444091\n",
      "  (0, 587)\t0.07537783614444091\n",
      "  (0, 2326)\t0.15075567228888181\n",
      "  (0, 3062)\t0.07537783614444091\n",
      "  (0, 3166)\t0.07537783614444091\n",
      "  (0, 4017)\t0.15075567228888181\n",
      "  (0, 4378)\t0.07537783614444091\n",
      "  (0, 4808)\t0.07537783614444091\n",
      "  (0, 5195)\t0.07537783614444091\n",
      "  (0, 5201)\t0.07537783614444091\n",
      "  (0, 5285)\t0.07537783614444091\n",
      "  (0, 8696)\t0.30151134457776363\n",
      "  (0, 9031)\t0.22613350843332272\n",
      "  (0, 9338)\t0.07537783614444091\n",
      "  (0, 9801)\t0.07537783614444091\n",
      "  (0, 9805)\t0.15075567228888181\n",
      "  (0, 9932)\t0.07537783614444091\n",
      "  (0, 12014)\t0.07537783614444091\n",
      "  (0, 12051)\t0.07537783614444091\n",
      "  (0, 12541)\t0.07537783614444091\n",
      "  (0, 12833)\t0.15075567228888181\n",
      "  (0, 14085)\t0.07537783614444091\n",
      "  (0, 14281)\t0.15075567228888181\n",
      "  (0, 14676)\t0.07537783614444091\n",
      "  :\t:\n",
      "  (2256, 24052)\t0.07216878364870323\n",
      "  (2256, 25560)\t0.07216878364870323\n",
      "  (2256, 26254)\t0.07216878364870323\n",
      "  (2256, 26998)\t0.07216878364870323\n",
      "  (2256, 27031)\t0.14433756729740646\n",
      "  (2256, 27042)\t0.07216878364870323\n",
      "  (2256, 28900)\t0.07216878364870323\n",
      "  (2256, 29224)\t0.07216878364870323\n",
      "  (2256, 29993)\t0.07216878364870323\n",
      "  (2256, 30325)\t0.21650635094610968\n",
      "  (2256, 30498)\t0.07216878364870323\n",
      "  (2256, 30776)\t0.21650635094610968\n",
      "  (2256, 31077)\t0.07216878364870323\n",
      "  (2256, 31342)\t0.07216878364870323\n",
      "  (2256, 31945)\t0.07216878364870323\n",
      "  (2256, 32142)\t0.43301270189221935\n",
      "  (2256, 32233)\t0.07216878364870323\n",
      "  (2256, 32270)\t0.07216878364870323\n",
      "  (2256, 33078)\t0.07216878364870323\n",
      "  (2256, 33844)\t0.07216878364870323\n",
      "  (2256, 34923)\t0.07216878364870323\n",
      "  (2256, 35157)\t0.07216878364870323\n",
      "  (2256, 35350)\t0.07216878364870323\n",
      "  (2256, 35638)\t0.14433756729740646\n",
      "  (2256, 35690)\t0.07216878364870323\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "\n",
    "print(X_train_tf.shape)\n",
    "print(X_train_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8348868175765646"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "    categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9101198402130493"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=5, tol=None)),\n",
    "])\n",
    "\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "Y = np.array([1, 1, 2, 2])\n",
    "# Always scale the input. The most convenient way is to use a pipeline.\n",
    "clf = make_pipeline(StandardScaler(),\n",
    "                    SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "clf.fit(X, Y)\n",
    "\n",
    "\n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.95      0.80      0.87       319\n",
      "         comp.graphics       0.87      0.98      0.92       389\n",
      "               sci.med       0.94      0.89      0.91       396\n",
      "soc.religion.christian       0.90      0.95      0.93       398\n",
      "\n",
      "              accuracy                           0.91      1502\n",
      "             macro avg       0.91      0.91      0.91      1502\n",
      "          weighted avg       0.91      0.91      0.91      1502\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[256,  11,  16,  36],\n",
       "       [  4, 380,   3,   2],\n",
       "       [  5,  35, 353,   3],\n",
       "       [  5,  11,   4, 378]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "    target_names=twenty_test.target_names))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf__alpha': (1e-2, 1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soc.religion.christian'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names[gs_clf.predict(['God is love'])[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9175000000000001\n",
      "clf__alpha: 0.001\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(gs_clf.best_score_)\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.32924027, 1.20088634, 0.41287451, 1.19522009, 0.36778102,\n",
       "        0.99673419, 0.3700253 , 1.04732394]),\n",
       " 'std_fit_time': array([0.06558295, 0.24057152, 0.07146008, 0.250734  , 0.11640576,\n",
       "        0.23146202, 0.05939982, 0.1115988 ]),\n",
       " 'mean_score_time': array([0.06630526, 0.17927408, 0.09450994, 0.14293246, 0.07306042,\n",
       "        0.13382945, 0.06679964, 0.10440493]),\n",
       " 'std_score_time': array([0.01279285, 0.06349584, 0.05885188, 0.04860669, 0.02882144,\n",
       "        0.04256523, 0.01776886, 0.01411977]),\n",
       " 'param_clf__alpha': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.001, 0.001, 0.001, 0.001],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_tfidf__use_idf': masked_array(data=[True, True, False, False, True, True, False, False],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__ngram_range': masked_array(data=[(1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2)],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'clf__alpha': 0.01,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)},\n",
       "  {'clf__alpha': 0.001, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)},\n",
       "  {'clf__alpha': 0.001, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}],\n",
       " 'split0_test_score': array([0.9   , 0.8875, 0.7   , 0.6875, 0.9   , 0.9125, 0.825 , 0.8   ]),\n",
       " 'split1_test_score': array([0.8625, 0.85  , 0.7125, 0.7   , 0.9   , 0.8875, 0.7625, 0.8   ]),\n",
       " 'split2_test_score': array([0.8875, 0.9   , 0.775 , 0.8   , 0.925 , 0.8875, 0.75  , 0.8   ]),\n",
       " 'split3_test_score': array([0.9125, 0.925 , 0.8   , 0.8125, 0.925 , 0.925 , 0.8   , 0.85  ]),\n",
       " 'split4_test_score': array([0.9   , 0.925 , 0.825 , 0.8375, 0.9375, 0.95  , 0.825 , 0.9   ]),\n",
       " 'mean_test_score': array([0.8925, 0.8975, 0.7625, 0.7675, 0.9175, 0.9125, 0.7925, 0.83  ]),\n",
       " 'std_test_score': array([0.01695582, 0.02783882, 0.04873397, 0.06154267, 0.015     ,\n",
       "        0.02371708, 0.03122499, 0.04      ]),\n",
       " 'rank_test_score': array([4, 3, 8, 7, 1, 2, 6, 5])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
